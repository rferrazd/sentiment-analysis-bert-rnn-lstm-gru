{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1bc8d8b",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "Implementation based on code from notebook: 1. Encoder-Decoder Seq2Seq.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, pickle, json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import nltk\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check the available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the datasets\n",
    "with open(\"data/train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"data/val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"data/test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "# Remap original labels {0,4} -> contiguous ids {0,1} for modeling\n",
    "# (Keep a copy of the original labels for reporting/debugging.)\n",
    "LABEL_MAP = {0: 0, 4: 1}\n",
    "\n",
    "for df_name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "    df[\"label_original\"] = df[\"label\"]\n",
    "    mapped = df[\"label_original\"].map(LABEL_MAP)\n",
    "\n",
    "    df[\"label\"] = mapped.astype(int)\n",
    "\n",
    "num_labels = train[\"label\"].nunique()\n",
    "print(\"Number of labels (label): \", num_labels)\n",
    "\n",
    "# With remapping, class ids are stable\n",
    "label_0 = 0  # Negative\n",
    "label_4 = 1  # Positive\n",
    "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb031e",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25467df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Text Preprocessing and Vocabulary Building\n",
    "# ---------------------------------------\n",
    "from collections import Counter\n",
    "\n",
    "# Token dictionary was already created in lstm.ipynb\n",
    "with open('tokens2ids.json', 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "with open('ids2tokens.json', 'r') as f:\n",
    "    idx_to_word = json.load(f)\n",
    "\n",
    "# -------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -------------------------\n",
    "# Function to tokenize text (same as in RNN notebook)\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Convert text to lowercase tokens.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Keep only letters and whitespaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    # Tokenize using NLTK\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "def tokens_to_indices(tokens, vocab, max_length=None):\n",
    "    \"\"\"Convert list of tokens to list of indices.\"\"\"\n",
    "    indices = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    if max_length:\n",
    "        # Truncate or pad to max_length\n",
    "        if len(indices) > max_length:\n",
    "            indices = indices[:max_length]\n",
    "        else:\n",
    "            indices = indices + [vocab['<PAD>']] * (max_length - len(indices))\n",
    "    return indices\n",
    "# -------------------------\n",
    "max_seq_length = 100  # Same as in the notebook Encoder-Decoder Seq2Seq.ipynb (MAX_LENGTH_INPUT = 100)\n",
    "\n",
    "# Tokenize all datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "train['tokens'] = train['text of the tweet'].apply(preprocess_text)\n",
    "val['tokens'] = val['text of the tweet'].apply(preprocess_text)\n",
    "test['tokens'] = test['text of the tweet'].apply(preprocess_text)\n",
    "\n",
    "# Convert all datasets to sequences of indices\n",
    "print(\"Converting text to sequences of indices...\")\n",
    "train['sequences'] = train['tokens'].apply(\n",
    "    lambda tokens: tokens_to_indices(tokens, vocab, max_seq_length)\n",
    ")\n",
    "val['sequences'] = val['tokens'].apply(\n",
    "    lambda tokens: tokens_to_indices(tokens, vocab, max_seq_length)\n",
    ")\n",
    "test['sequences'] = test['tokens'].apply(\n",
    "    lambda tokens: tokens_to_indices(tokens, vocab, max_seq_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a951dc",
   "metadata": {},
   "source": [
    "# Bi-Directional GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "class BiGRU_Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A Bidirectional GRU-based module designed for sequence encoding\n",
    "    and subsequent sentiment classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_labels, dropout_p=0.1):\n",
    "        super(BiGRU_Classifier, self).__init__()\n",
    "        \n",
    "        # --- Parameters ---\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_labels\n",
    "        self.num_directions = 2  # Fixed for BiGRU\n",
    "\n",
    "        # --- Embedding Layer ---\n",
    "        # input: (batch_size, seq_len) -> indices of tokens\n",
    "        # output: (batch_size, seq_len, embedding_dim) -> dense word vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # --- Dropout Layer ---\n",
    "        # Applied after embedding to regularize word vectors (Embedding Dropout)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # --- BiGRU Layer ---\n",
    "        # input_size: embedding_dim (the size of the input features per time step)\n",
    "        # hidden_size: hidden_size (the output size of the hidden state for ONE direction)\n",
    "        # batch_first=True: input shape is (batch_size, seq_len, features)\n",
    "        # bidirectional=True: output_dim = 2 * hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # --- Classification Layer (MLP) ---\n",
    "        # The BiGRU combines the final forward and backward hidden states.\n",
    "        # Input size to the Linear layer must be (2 * hidden_size)\n",
    "        # Output size is num_labels (e.g., 2 for positive/negative)\n",
    "        self.classifier = nn.Linear(self.num_directions * hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # 1. Embedding\n",
    "        # shape: (bs, seq_len) -> (bs, seq_len, embedding_dim)\n",
    "        embedded = self.dropout(self.embedding(input_tensor))\n",
    "    \n",
    "        # 2. BiGRU Processing\n",
    "        # output: (bs, seq_len, 2 * hidden_size) - full sequence output\n",
    "        # hidden: state from both directions, shape: (2, bs, hidden_size)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        \n",
    "        # 3. Aggregate Hidden States for Classification\n",
    "        # Reshape hidden to (num_directions, batch_size, hidden_size)\n",
    "        hidden = hidden.view(self.num_directions, -1, self.hidden_size)\n",
    "        \n",
    "        # Concatenate forward (hidden[-2]) and backward (hidden[-1]) final states\n",
    "        final_hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        \n",
    "        # 4. Final Classification\n",
    "        # input: (bs, 2 * hidden_size)\n",
    "        # output: (bs, num_classes)\n",
    "        prediction_logits = self.classifier(final_hidden)\n",
    "        return prediction_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128  # same as in notebook: 1. Encoder-Decoder Seq2Seq.ipynb\n",
    "hidden_size = 128    # Common values: 64, 128, 256 - maybe decrease if training is too slow\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "print(f\"Model hyperparameters:\")\n",
    "print(f\"  vocab_size: {vocab_size:,}\")\n",
    "print(f\"  embedding_dim: {embedding_dim}\")\n",
    "print(f\"  hidden_size: {hidden_size}\")\n",
    "print(f\"  num_labels: {num_labels}\")\n",
    "\n",
    "# Initialize model\n",
    "model = BiGRU_Classifier(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size, \n",
    "    num_labels=num_labels\n",
    ").to(device)\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification (2 classes: negative=0, positive=1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Prepare DataLoaders\n",
    "# ---------------------------------------\n",
    "\n",
    "# Convert sequences and labels to tensors\n",
    "X_train = torch.tensor(train['sequences'].tolist(), dtype=torch.long)\n",
    "y_train = torch.tensor(train['label'].values, dtype=torch.long)\n",
    "\n",
    "X_val = torch.tensor(val['sequences'].tolist(), dtype=torch.long)\n",
    "y_val = torch.tensor(val['label'].values, dtype=torch.long)\n",
    "\n",
    "X_test = torch.tensor(test['sequences'].tolist(), dtype=torch.long)\n",
    "y_test = torch.tensor(test['label'].values, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73510ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create mini versions of the train and val datasets (first 5 examples)\n",
    "# mini_train = train[:5]\n",
    "# mini_val = val[:5]\n",
    "\n",
    "# X_train_mini = torch.tensor(mini_train['sequences'].tolist(), dtype=torch.long)\n",
    "# y_train_mini = torch.tensor(mini_train['label'].values, dtype=torch.long)\n",
    "\n",
    "# X_val_mini = torch.tensor(mini_val['sequences'].tolist(), dtype=torch.long)\n",
    "# y_val_mini = torch.tensor(mini_val['label'].values, dtype=torch.long)\n",
    "\n",
    "# mini_train_dataset = TensorDataset(X_train_mini, y_train_mini)\n",
    "# mini_val_dataset = TensorDataset(X_val_mini, y_val_mini)\n",
    "\n",
    "# mini_train_loader = DataLoader(mini_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# mini_val_loader = DataLoader(mini_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print(f\"MINI - Train batches: {len(mini_train_loader)}\")\n",
    "# print(f\"MINI - Val batches: {len(mini_val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43750e88",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "patience = 0\n",
    "max_patience = 3\n",
    "\n",
    "# Start timing training\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # inputs.shape = [bs, max_seq_len]\n",
    "        inputs = inputs.to(device)\n",
    "        # labels.shape = [bs]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # logits.shape: [batch_size, num_labels]\n",
    "        logits = model(inputs)  \n",
    "        # loss.shape: [batch_size] with class indices\n",
    "        loss = criterion(logits, labels)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        count += 1\n",
    "    \n",
    "    average_loss = total_loss / max(count, 1)\n",
    "    losses.append(average_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)  # Shape: [batch_size, 2]\n",
    "            val_loss = criterion(logits, labels)  # labels: [batch_size] with class indices\n",
    "\n",
    "            total_val_loss += float(val_loss.item())\n",
    "            val_count += 1\n",
    "\n",
    "    average_val_loss = total_val_loss / max(val_count, 1)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"gru_best_model.pth\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f\"Early stopped at {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Calculate total training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}\")\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbb789",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca800ab",
   "metadata": {},
   "source": [
    "## Evaluation Functions - Accuracy, Label Precision, Recall, F1, and Confusion Matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eeba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764e5de",
   "metadata": {},
   "source": [
    "## Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ed3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and calculate accuracy only for that\n",
    "model.load_state_dict(torch.load(\"gru_best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# Calculate number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# After training, evaluate on validation set (with confusion matrix plot)\n",
    "val_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    val_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Validation Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "    num_trainable_parameters=num_trainable_params,\n",
    "    model_name=\"gru\",\n",
    "    dataset_split=\"val\",\n",
    "    save_results=True,\n",
    ")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
    "print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa77bb9",
   "metadata": {},
   "source": [
    "##  Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d77286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (with confusion matrix plot)\n",
    "test_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    test_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Test Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "    num_trainable_parameters=num_trainable_params,\n",
    "    model_name=\"gru\",\n",
    "    dataset_split=\"test\",\n",
    "    save_results=True,\n",
    ")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
