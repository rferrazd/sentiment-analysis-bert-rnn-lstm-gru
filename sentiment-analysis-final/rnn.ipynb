{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82de422e",
      "metadata": {},
      "source": [
        "# RNN\n",
        "\n",
        "*INSTRUCTIONS*\n",
        "Embedding for RNN-based Models:\n",
        "    ○ Generate wordembeddings using GloVeorWord2Vec.\n",
        "    ○ Pad sequences to a fixed length for uniformity\n",
        "\n",
        "Steps were taken from notebook: Module 3 - Video 6 onwards.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f5ace5e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Number of labels (label):  2\n",
            "Label 0: 0 and label 4: 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity of tweet</th>\n",
              "      <th>id of the tweet</th>\n",
              "      <th>date of the tweet</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text of the tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>237034</th>\n",
              "      <td>0</td>\n",
              "      <td>2058468667</td>\n",
              "      <td>Sat Jun 06 15:00:18 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bestthingaround</td>\n",
              "      <td>my star trek bootleg timed out and when i refr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387008</th>\n",
              "      <td>0</td>\n",
              "      <td>2068651245</td>\n",
              "      <td>Sun Jun 07 14:27:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Scriblit</td>\n",
              "      <td>yeah but the really pretty ones only go up to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
              "237034                   0       2058468667  Sat Jun 06 15:00:18 PDT 2009   \n",
              "1387008                  0       2068651245  Sun Jun 07 14:27:20 PDT 2009   \n",
              "\n",
              "            query             user  \\\n",
              "237034   NO_QUERY  bestthingaround   \n",
              "1387008  NO_QUERY         Scriblit   \n",
              "\n",
              "                                         text of the tweet  label  \\\n",
              "237034   my star trek bootleg timed out and when i refr...      0   \n",
              "1387008  yeah but the really pretty ones only go up to ...      0   \n",
              "\n",
              "         label_original  \n",
              "237034                0  \n",
              "1387008               0  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from gensim.models import Word2Vec\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Check the available device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the datasets\n",
        "with open(\"data/train.pkl\", \"rb\") as f:\n",
        "    train = pickle.load(f)\n",
        "with open(\"data/val.pkl\", \"rb\") as f:\n",
        "    val = pickle.load(f)\n",
        "with open(\"data/test.pkl\", \"rb\") as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "# Remap original labels {0,4} -> contiguous ids {0,1} for modeling\n",
        "# (Keep a copy of the original labels for reporting/debugging.)\n",
        "LABEL_MAP = {0: 0, 4: 1}\n",
        "\n",
        "for df_name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
        "    df[\"label_original\"] = df[\"label\"]\n",
        "    mapped = df[\"label_original\"].map(LABEL_MAP)\n",
        "\n",
        "    if mapped.isna().any():\n",
        "        bad = sorted(df.loc[mapped.isna(), \"label_original\"].unique().tolist())\n",
        "        raise ValueError(f\"Unexpected labels in {df_name}: {bad}\")\n",
        "\n",
        "    df[\"label\"] = mapped.astype(int)\n",
        "\n",
        "num_labels = train[\"label\"].nunique()\n",
        "print(\"Number of labels (label): \", num_labels)\n",
        "\n",
        "# With remapping, class ids are stable\n",
        "label_0 = 0  # Negative\n",
        "label_4 = 1  # Positive\n",
        "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
        "\n",
        "train.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edec17c5",
      "metadata": {},
      "source": [
        "# Utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d122a645",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Utils\n",
        "# ---------------------------------------\n",
        "\n",
        "# Function to convert text to tokens\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # Keep only letters and whitespaces (avoid regex escape warnings)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Function to convert tokens to Word2Vec embeddings\n",
        "def text_to_embeddings(text, word2vec_model, seq_length):\n",
        "    \"\"\"\n",
        "    Function to convert a given token list into a sequence of embeddings using a pretrained Word2Vec model.\n",
        "    Optimized version: pre-allocates numpy array (padding handled automatically with zeros).\n",
        "    Uses float16 to reduce memory usage.\n",
        "    \"\"\"\n",
        "    vector_size = word2vec_model.vector_size\n",
        "    # Pre-allocate array filled with zeros - this automatically handles padding!\n",
        "    # If text is shorter than seq_length, remaining positions stay as zeros (padding)\n",
        "    embeddings = np.zeros((seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "    idx = 0\n",
        "    for word in text[:seq_length]:  # Only process up to seq_length\n",
        "        if word in word2vec_model.wv:\n",
        "            # Convert to float16 on assignment (in case underlying vectors are float32)\n",
        "            embeddings[idx] = word2vec_model.wv[word].astype(np.float16)\n",
        "            idx += 1\n",
        "        \n",
        "        # if idx < seq_length, remaining positions are already zeros (padding)\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Text -> Embeddings -> torch tensors\n",
        "def prepare_data(reviews, labels, word2vec_model, seq_length):\n",
        "    \"\"\"\n",
        "    Optimized version: pre-allocates output array and processes in batch.\n",
        "    Uses float16 for X_array and resulting torch tensor.\n",
        "    \"\"\"\n",
        "    num_samples = len(reviews)\n",
        "    vector_size = word2vec_model.vector_size\n",
        "    \n",
        "    # Pre-allocate output array using float16 (saves memory)\n",
        "    X_array = np.zeros((num_samples, seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "    # Fill array directly (faster than building list)\n",
        "    for i, review in enumerate(reviews):\n",
        "        X_array[i] = text_to_embeddings(review, word2vec_model, seq_length)\n",
        "    \n",
        "    # Convert to tensors (from_numpy is faster and shares memory)\n",
        "    X_tensor = torch.from_numpy(X_array).to(torch.float16)\n",
        "    \n",
        "    # Convert labels to numpy array first (handles pandas Series)\n",
        "    if hasattr(labels, 'values'):\n",
        "        labels = labels.values\n",
        "    y = torch.from_numpy(np.array(labels))\n",
        "    \n",
        "    return X_tensor, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9132b7c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1072b94",
      "metadata": {},
      "source": [
        "# Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9864ab6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "DIR_NAME = \"word2vec_tokens_data\"\n",
        "zip_path = f\"{DIR_NAME}.zip\"\n",
        "extract_dir = DIR_NAME\n",
        "\n",
        "# Extract if zip exists but folder doesn't\n",
        "# if os.path.exists(zip_path) and not os.path.exists(extract_dir):\n",
        "#     print(f\"Found {zip_path} - extracting to {extract_dir}...\")\n",
        "#     with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "#         zip_ref.extractall(extract_dir)\n",
        "#     print(\"Extraction complete.\")\n",
        "\n",
        "# If the folder exists, compress its contents into a zip file (no subdirs, just .pkl files and direct children)\n",
        "# if os.path.exists(extract_dir) and not os.path.exists(zip_path):\n",
        "#     print(f\"Zipping files from {extract_dir} into {zip_path} (flat, no folders)...\")\n",
        "#     with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "#         for filename in os.listdir(extract_dir):\n",
        "#             file_path = os.path.join(extract_dir, filename)\n",
        "#             if os.path.isfile(file_path):\n",
        "#                 zipf.write(file_path, arcname=filename)\n",
        "#     print(\"Zip file creation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a172ded3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenized data from cache...\n",
            "Tokenized data loaded from cache!\n",
            "Loading Word2Vec model from cache...\n",
            "Word2Vec model loaded from cache!\n",
            "Vocab size:  256064\n"
          ]
        }
      ],
      "source": [
        "# Save this code in a separate file named word2vec_tokens.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "def load_or_create_tokens(train, val, test, preprocess_text, dir_name=DIR_NAME):\n",
        "    \"\"\"\n",
        "    Tokenizes tweet texts and caches the result, or loads from cache if available.\n",
        "    Token files are saved/loaded from '{dir_name}/tokens_train.pkl' etc.\n",
        "    \"\"\"\n",
        "    train_tokens_path = os.path.join(dir_name, \"tokens_train.pkl\")\n",
        "    val_tokens_path = os.path.join(dir_name, \"tokens_val.pkl\")\n",
        "    test_tokens_path = os.path.join(dir_name, \"tokens_test.pkl\")\n",
        "    if os.path.exists(train_tokens_path) and os.path.exists(val_tokens_path) and os.path.exists(test_tokens_path):\n",
        "        print(\"Loading tokenized data from cache...\")\n",
        "        with open(train_tokens_path, \"rb\") as f:\n",
        "            train['tokens'] = pickle.load(f)\n",
        "        with open(val_tokens_path, \"rb\") as f:\n",
        "            val['tokens'] = pickle.load(f)\n",
        "        with open(test_tokens_path, \"rb\") as f:\n",
        "            test['tokens'] = pickle.load(f)\n",
        "        print(\"Tokenized data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Tokenizing data (this may take a while)...\")\n",
        "        train['tokens'] = train['text of the tweet'].apply(preprocess_text)\n",
        "        val['tokens'] = val['text of the tweet'].apply(preprocess_text)\n",
        "        test['tokens'] = test['text of the tweet'].apply(preprocess_text)\n",
        "        \n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        # Save tokenized data\n",
        "        with open(train_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(train['tokens'], f)\n",
        "        with open(val_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(val['tokens'], f)\n",
        "        with open(test_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(test['tokens'], f)\n",
        "        print(\"Tokenized data saved to cache!\")\n",
        "    return train, val, test\n",
        "\n",
        "def load_or_train_word2vec(train, seq_length, dir_name=DIR_NAME):\n",
        "    \"\"\"\n",
        "    Loads a cached Word2Vec model, or trains and caches a new one.\n",
        "    The model is always saved/loaded as '{dir_name}/word2vec_model.model.'\n",
        "    \"\"\"\n",
        "    model_path = os.path.join(dir_name, \"word2vec_model.model\")\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"Loading Word2Vec model from cache...\")\n",
        "        word2vec_model = Word2Vec.load(model_path)\n",
        "        print(\"Word2Vec model loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Training Word2Vec model (this may take a while)...\")\n",
        "        word2vec_model = Word2Vec(sentences=train['tokens'].values.tolist(),\n",
        "                                  vector_size=seq_length,\n",
        "                                  min_count=1,\n",
        "                                  workers=4)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        word2vec_model.save(model_path)\n",
        "        print(\"Word2Vec model saved to cache!\")\n",
        "    vocab_size = len(word2vec_model.wv)\n",
        "    print(\"Vocab size: \", vocab_size)\n",
        "    return word2vec_model, vocab_size\n",
        "\n",
        "\n",
        "train, val, test = load_or_create_tokens(train, val, test, preprocess_text)\n",
        "seq_length = 100\n",
        "word2vec_model, vocab_size = load_or_train_word2vec(train, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c352b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading preprocessed training data from cache...\n",
            "Training data loaded from cache!\n",
            "Preprocessing validation data (this may take a while)...\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "input_size = word2vec_model.vector_size\n",
        "hidden_size = 128\n",
        "output_size = 2 \n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "dropout_rate = 0.5\n",
        "leaky_relu_slope = 0.1\n",
        "\n",
        "\n",
        "def load_or_prepare_data(dir_name, train, val, test, word2vec_model, seq_length):\n",
        "    \"\"\"\n",
        "    Loads or creates preprocessed embeddings for train, val, and test sets.\n",
        "    Returns all six tensors: X_train, y_train, X_val, y_val, X_test, y_test\n",
        "    \"\"\"\n",
        "    # TRAIN\n",
        "    X_train_path = os.path.join(dir_name, \"X_train.pt\")\n",
        "    y_train_path = os.path.join(dir_name, \"y_train.pt\")\n",
        "    if os.path.exists(X_train_path) and os.path.exists(y_train_path):\n",
        "        print(\"Loading preprocessed training data from cache...\")\n",
        "        X_train = torch.load(X_train_path, map_location='cpu')\n",
        "        y_train = torch.load(y_train_path, map_location='cpu')\n",
        "        print(\"Training data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing training data (this may take a while)...\")\n",
        "        X_train, y_train = prepare_data(train['tokens'], train['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_train, X_train_path)\n",
        "        torch.save(y_train, y_train_path)\n",
        "        print(\"Training data saved to cache!\")\n",
        "\n",
        "    # VALIDATION\n",
        "    X_val_path = os.path.join(dir_name, \"X_val.pt\")\n",
        "    y_val_path = os.path.join(dir_name, \"y_val.pt\")\n",
        "    if os.path.exists(X_val_path) and os.path.exists(y_val_path):\n",
        "        print(\"Loading preprocessed validation data from cache...\")\n",
        "        X_val = torch.load(X_val_path, map_location='cpu')\n",
        "        y_val = torch.load(y_val_path, map_location='cpu')\n",
        "        print(\"Validation data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing validation data (this may take a while)...\")\n",
        "        X_val, y_val = prepare_data(val['tokens'], val['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_val, X_val_path)\n",
        "        torch.save(y_val, y_val_path)\n",
        "        print(\"Validation data saved to cache!\")\n",
        "\n",
        "    # TEST\n",
        "    X_test_path = os.path.join(dir_name, \"X_test.pt\")\n",
        "    y_test_path = os.path.join(dir_name, \"y_test.pt\")\n",
        "    if os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
        "        print(\"Loading preprocessed test data from cache...\")\n",
        "        X_test = torch.load(X_test_path, map_location='cpu')\n",
        "        y_test = torch.load(y_test_path, map_location='cpu')\n",
        "        print(\"Test data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing test data (this may take a while)...\")\n",
        "        X_test, y_test = prepare_data(test['tokens'], test['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_test, X_test_path)\n",
        "        torch.save(y_test, y_test_path)\n",
        "        print(\"Test data saved to cache!\")\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Prepare data - load or create all embeddings\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_or_prepare_data(\n",
        "    DIR_NAME, train, val, test, word2vec_model, seq_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71996079",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc # garbage collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9192ac0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150220c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f632f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48cdd93f",
      "metadata": {},
      "source": [
        "# Define the RNN model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9622ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope=0.1):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # Basic RNN layer\n",
        "        # shape of input tensor: (batch_size, seq_length, input_size)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0,\n",
        "        )\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial hidden state\n",
        "        # h0 shape: (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "\n",
        "        # out shape after rnn: (batch_size, seq_length, hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Get the last sequence output for classification\n",
        "        # out shape after indexing: (batch_size, hidden_size)\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SentimentRNN(\n",
        "    input_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dropout_rate,\n",
        "    leaky_relu_slope=leaky_relu_slope,\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb65eef",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ce77d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "num_epochs = 30  # Number of epochs\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = 0\n",
        "patience = 0\n",
        "max_patience = 3\n",
        "\n",
        "# Start timing training\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # CrossEntropyLoss expects long integer class indices (0 or 1)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)  # Shape: (batch_size, 2)\n",
        "        loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "        count += 1\n",
        "\n",
        "    average_loss = total_loss / max(count, 1)\n",
        "    losses.append(average_loss)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    val_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "    \n",
        "            logits = model(inputs)  # Shape: (batch_size, 2)\n",
        "            val_loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
        "\n",
        "            total_val_loss += float(val_loss.item())\n",
        "            val_count += 1\n",
        "\n",
        "    average_val_loss = total_val_loss / max(val_count, 1)\n",
        "    val_losses.append(average_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
        "\n",
        "    # Save best checkpoint\n",
        "    if average_val_loss < best_val_loss:\n",
        "        best_val_loss = average_val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"rnn_best_model.pth\")\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    if patience >= max_patience:\n",
        "        print(f\"Early stopped at {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Calculate total training time\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}\")\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c1bf43",
      "metadata": {},
      "source": [
        "# Evaluate "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b258fa8",
      "metadata": {},
      "source": [
        "## Evaluation Functions - Accuracy, Label Precision, Recall, F1, and Confusion Matric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b16bac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics import evaluate_model\n",
        "\n",
        "# Calculate number of trainable parameters\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f72d494",
      "metadata": {},
      "source": [
        "## Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407117aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model and calculate accuracy only for that\n",
        "model.load_state_dict(torch.load(\"rnn_best_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# After training, evaluate on validation set (with confusion matrix plot)\n",
        "val_metrics = evaluate_model(\n",
        "    model, \n",
        "    device, \n",
        "    val_loader,\n",
        "    label_0,\n",
        "    label_4,\n",
        "    plot_confusion_matrix=True,\n",
        "    title=\"Validation Set Confusion Matrix\",\n",
        "    training_time_seconds=training_time,\n",
        "    num_trainable_parameters=num_trainable_params,\n",
        "    model_name=\"rnn\",\n",
        "    dataset_split=\"val\",\n",
        "    save_results=True,\n",
        ")\n",
        "\n",
        "print(f\"Validation Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
        "print(f\"Validation Metrics: {val_metrics}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "219bab26",
      "metadata": {},
      "source": [
        "## Test Set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad0b959",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set (without plot)\n",
        "\n",
        "# Or plot test confusion matrix separately\n",
        "test_metrics = evaluate_model(\n",
        "    model, \n",
        "    device, \n",
        "    test_loader,\n",
        "    label_0,\n",
        "    label_4,\n",
        "    plot_confusion_matrix=True,\n",
        "    title=\"Test Set Confusion Matrix\",\n",
        "    training_time_seconds=training_time,\n",
        "     num_trainable_parameters=num_trainable_params,\n",
        "    model_name=\"rnn\",\n",
        "    dataset_split=\"test\",\n",
        "    save_results=True,\n",
        ")\n",
        "\n",
        "print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
