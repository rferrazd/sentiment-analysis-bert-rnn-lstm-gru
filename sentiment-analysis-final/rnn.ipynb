{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82de422e",
      "metadata": {},
      "source": [
        "# RNN\n",
        "\n",
        "*INSTRUCTIONS*\n",
        "Embedding for RNN-based Models:\n",
        "    ○ Generate wordembeddings using GloVeorWord2Vec.\n",
        "    ○ Pad sequences to a fixed length for uniformity\n",
        "\n",
        "Steps were taken from notebook: Module 3 - Video 6 onwards.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f5ace5e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Number of labels (label):  2\n",
            "Label 0: 0 and label 4: 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity of tweet</th>\n",
              "      <th>id of the tweet</th>\n",
              "      <th>date of the tweet</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text of the tweet</th>\n",
              "      <th>label</th>\n",
              "      <th>label_original</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>237034</th>\n",
              "      <td>0</td>\n",
              "      <td>2058468667</td>\n",
              "      <td>Sat Jun 06 15:00:18 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>bestthingaround</td>\n",
              "      <td>my star trek bootleg timed out and when i refr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387008</th>\n",
              "      <td>0</td>\n",
              "      <td>2068651245</td>\n",
              "      <td>Sun Jun 07 14:27:20 PDT 2009</td>\n",
              "      <td>NO_QUERY</td>\n",
              "      <td>Scriblit</td>\n",
              "      <td>yeah but the really pretty ones only go up to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
              "237034                   0       2058468667  Sat Jun 06 15:00:18 PDT 2009   \n",
              "1387008                  0       2068651245  Sun Jun 07 14:27:20 PDT 2009   \n",
              "\n",
              "            query             user  \\\n",
              "237034   NO_QUERY  bestthingaround   \n",
              "1387008  NO_QUERY         Scriblit   \n",
              "\n",
              "                                         text of the tweet  label  \\\n",
              "237034   my star trek bootleg timed out and when i refr...      0   \n",
              "1387008  yeah but the really pretty ones only go up to ...      0   \n",
              "\n",
              "         label_original  \n",
              "237034                0  \n",
              "1387008               0  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import gc # garbage collection\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import torch\n",
        "from gensim.models import Word2Vec\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# Check the available device\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the datasets\n",
        "with open(\"data/train.pkl\", \"rb\") as f:\n",
        "    og_train = pickle.load(f)\n",
        "with open(\"data/val.pkl\", \"rb\") as f:\n",
        "    val = pickle.load(f)\n",
        "with open(\"data/test.pkl\", \"rb\") as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "# Remap original labels {0,4} -> contiguous ids {0,1} for modeling\n",
        "# (Keep a copy of the original labels for reporting/debugging.)\n",
        "LABEL_MAP = {0: 0, 4: 1}\n",
        "\n",
        "for df_name, df in [(\"train\", og_train), (\"val\", val), (\"test\", test)]:\n",
        "    df[\"label_original\"] = df[\"label\"]\n",
        "    mapped = df[\"label_original\"].map(LABEL_MAP)\n",
        "\n",
        "    if mapped.isna().any():\n",
        "        bad = sorted(df.loc[mapped.isna(), \"label_original\"].unique().tolist())\n",
        "        raise ValueError(f\"Unexpected labels in {df_name}: {bad}\")\n",
        "\n",
        "    df[\"label\"] = mapped.astype(int)\n",
        "\n",
        "num_labels = og_train[\"label\"].nunique()\n",
        "print(\"Number of labels (label): \", num_labels)\n",
        "\n",
        "# With remapping, class ids are stable\n",
        "label_0 = 0  # Negative\n",
        "label_4 = 1  # Positive\n",
        "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
        "\n",
        "og_train.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5fbd4f14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1151993, 8) (287999, 8) (160000, 8)\n",
            "loaded train_slice: (1000000, 8)\n"
          ]
        }
      ],
      "source": [
        "print(og_train.shape, val.shape, test.shape)\n",
        "# Due to memory issues get a slice of the train data:\n",
        "# Select a random sample of 1 million rows from train and save to data/train_slice.pkl\n",
        "train_slice = og_train.sample(n=1_000_000, random_state=SEED)\n",
        "with open(\"data/train_slice.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_slice, f)\n",
        "\n",
        "with open(\"data/train_slice.pkl\", \"rb\") as f:\n",
        "    train = pickle.load(f)\n",
        "\n",
        "print(f\"loaded train_slice: {train.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edec17c5",
      "metadata": {},
      "source": [
        "# Utils "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c2b65cbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Core(s) per socket:                   4\n"
          ]
        }
      ],
      "source": [
        "# Check number of CPU cores available\n",
        "\n",
        "# quick viw of CPUs: \n",
        "    # !nproc\n",
        "# more detailed view: \n",
        "    # !lscpu\n",
        "\n",
        "# Physical CPU cores only (not threads)\n",
        "    # lscpu | grep \"^CPU(s):\"\n",
        "\n",
        "# Number of physical cores\n",
        "!lscpu | grep \"Core(s) per socket\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d122a645",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Utils - OD\n",
        "# ---------------------------------------\n",
        "# def has_repeated_chars(word, n):\n",
        "#     return any(all(word[i+j] == word[i] for j in range(n)) for i in range(len(word) - n + 1))\n",
        "\n",
        "\n",
        "# # Function to convert text to tokens\n",
        "# def preprocess_text(text):\n",
        "#     # Check if the text is a string\n",
        "#     if not isinstance(text, str):\n",
        "#         return []\n",
        "\n",
        "#     # remove words with more than 3 repeated characters\n",
        "#     # Keep only letters and whitespaces (avoid regex escape warnings)\n",
        "#     text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
        "#     text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "#     # Convert to lowercase\n",
        "#     text = text.lower()\n",
        "\n",
        "#     # Remove words with more than 3 same characters in a row\n",
        "#     text = \" \".join([word for word in text.split() if not(has_repeated_chars(word,3))])\n",
        "\n",
        "#     # Tokenize the text\n",
        "#     tokens = nltk.word_tokenize(text)\n",
        "\n",
        "#     return tokens\n",
        "\n",
        "# # Function to convert tokens to Word2Vec embeddings\n",
        "# def text_to_embeddings(text, word2vec_model, seq_length):\n",
        "#     \"\"\"\n",
        "#     Function to convert a given token list into a sequence of embeddings using a pretrained Word2Vec model.\n",
        "#     Optimized version: pre-allocates numpy array (padding handled automatically with zeros).\n",
        "#     Uses float16 to reduce memory usage.\n",
        "#     \"\"\"\n",
        "#     vector_size = word2vec_model.vector_size\n",
        "#     # Pre-allocate array filled with zeros - this automatically handles padding!\n",
        "#     # If text is shorter than seq_length, remaining positions stay as zeros (padding)\n",
        "#     embeddings = np.zeros((seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "#     idx = 0\n",
        "#     for word in text[:seq_length]:  # Only process up to seq_length\n",
        "#         if word in word2vec_model.wv:\n",
        "#             # Convert to float16 on assignment (in case underlying vectors are float32)\n",
        "#             embeddings[idx] = word2vec_model.wv[word].astype(np.float16)\n",
        "#             idx += 1\n",
        "        \n",
        "#         # if idx < seq_length, remaining positions are already zeros (padding)\n",
        "    \n",
        "#     return embeddings\n",
        "\n",
        "# # Text -> Embeddings -> torch tensors\n",
        "# def prepare_data(reviews, labels, word2vec_model, seq_length):\n",
        "#     \"\"\"\n",
        "#     Optimized version: pre-allocates output array and processes in batch.\n",
        "#     Uses float16 for X_array and resulting torch tensor.\n",
        "#     \"\"\"\n",
        "#     num_samples = len(reviews)\n",
        "#     vector_size = word2vec_model.vector_size\n",
        "    \n",
        "#     # Pre-allocate output array using float16 (saves memory)\n",
        "#     X_array = np.zeros((num_samples, seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "#     # Fill array directly (faster than building list)\n",
        "#     for i, review in enumerate(reviews):\n",
        "#         X_array[i] = text_to_embeddings(review, word2vec_model, seq_length)\n",
        "    \n",
        "#     # Convert to tensors (from_numpy is faster and shares memory)\n",
        "#     X_tensor = torch.from_numpy(X_array).to(torch.float16)\n",
        "    \n",
        "#     # Convert labels to numpy array first (handles pandas Series)\n",
        "#     if hasattr(labels, 'values'):\n",
        "#         labels = labels.values\n",
        "#     y = torch.from_numpy(np.array(labels))\n",
        "    \n",
        "#     return X_tensor, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9132b7c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------\n",
        "# Utils \n",
        "# ---------------------------------------\n",
        "\n",
        "# Parallel processing - need multiple CPU cores\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "\n",
        "def has_repeated_chars(word, n):\n",
        "    return any(all(word[i+j] == word[i] for j in range(n)) for i in range(len(word) - n + 1))\n",
        "\n",
        "# Function to convert text to tokens\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # remove words with more than 3 repeated characters\n",
        "    # Keep only letters and whitespaces (avoid regex escape warnings)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove words with more than 3 same characters in a row\n",
        "    text = \" \".join([word for word in text.split() if not(has_repeated_chars(word,3))])\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Optimized: Pre-build embedding matrix and vocabulary set for faster lookups\n",
        "def build_embedding_lookup(word2vec_model):\n",
        "    \"\"\"\n",
        "    Pre-builds embedding lookup structures for faster access.\n",
        "    Returns: vocab_set (set), embedding_matrix (numpy array), word_to_idx (dict)\n",
        "    \"\"\"\n",
        "    vocab_set = set(word2vec_model.wv.key_to_index.keys())\n",
        "    vector_size = word2vec_model.vector_size\n",
        "    \n",
        "    # Build embedding matrix (vocab_size x vector_size)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(word2vec_model.wv.key_to_index.keys())}\n",
        "    embedding_matrix = np.zeros((len(vocab_set), vector_size), dtype=np.float16)\n",
        "    \n",
        "    for word, idx in word_to_idx.items():\n",
        "        embedding_matrix[idx] = word2vec_model.wv[word].astype(np.float16)\n",
        "    \n",
        "    return vocab_set, embedding_matrix, word_to_idx\n",
        "\n",
        "# Optimized: Vectorized embedding conversion\n",
        "def text_to_embeddings_vectorized(tokens, vocab_set, embedding_matrix, word_to_idx, seq_length):\n",
        "    \"\"\"\n",
        "    Vectorized version: converts token list to embeddings using pre-built lookup structures.\n",
        "    Much faster than repeated dictionary lookups.\n",
        "    \"\"\"\n",
        "    vector_size = embedding_matrix.shape[1]\n",
        "    embeddings = np.zeros((seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "    idx = 0\n",
        "    for token in tokens[:seq_length]:\n",
        "        if token in vocab_set:\n",
        "            embeddings[idx] = embedding_matrix[word_to_idx[token]]\n",
        "            idx += 1\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Text -> Embeddings -> torch tensors\n",
        "# Text -> Embeddings -> torch tensors (Optimized Sequential Version)\n",
        "def prepare_data(reviews, labels, word2vec_model, seq_length, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Memory-efficient sequential version: processes in smaller chunks.\n",
        "    Uses pre-built lookup structures for faster embedding access.\n",
        "    Uses float16 for X_array and resulting torch tensor.\n",
        "    Smaller default chunk_size to reduce memory peaks.\n",
        "    \"\"\"\n",
        "    num_samples = len(reviews)\n",
        "    vector_size = word2vec_model.vector_size\n",
        "    \n",
        "    print(f\"Processing {num_samples} samples with chunk_size={chunk_size}...\")\n",
        "    \n",
        "    # Pre-build lookup structures (one-time cost)\n",
        "    print(\"Building embedding lookup structures...\")\n",
        "    vocab_set, embedding_matrix, word_to_idx = build_embedding_lookup(word2vec_model)\n",
        "    print(f\"Lookup structures built. Vocab size: {len(vocab_set)}\")\n",
        "    gc.collect()\n",
        "    \n",
        "    # Pre-allocate output array using float16 (saves memory)\n",
        "    print(f\"Allocating output array: {num_samples} x {seq_length} x {vector_size} (float16)...\")\n",
        "    X_array = np.zeros((num_samples, seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "    # Process in chunks to reduce memory peaks\n",
        "    num_chunks = (num_samples + chunk_size - 1) // chunk_size\n",
        "    print(f\"Processing in {num_chunks} chunks...\")\n",
        "    \n",
        "    for chunk_idx, chunk_start in enumerate(range(0, num_samples, chunk_size)):\n",
        "        chunk_end = min(chunk_start + chunk_size, num_samples)\n",
        "        \n",
        "        if (chunk_idx + 1) % 10 == 0 or chunk_idx == 0:\n",
        "            print(f\"Processing chunk {chunk_idx + 1}/{num_chunks} (samples {chunk_start}-{chunk_end})...\")\n",
        "        \n",
        "        # Process chunk\n",
        "        for i in range(chunk_start, chunk_end):\n",
        "            review_tokens = reviews.iloc[i] if hasattr(reviews, 'iloc') else reviews[i]\n",
        "            X_array[i] = text_to_embeddings_vectorized(\n",
        "                review_tokens, vocab_set, embedding_matrix, word_to_idx, seq_length\n",
        "            )\n",
        "        \n",
        "        # Clear cache after each chunk\n",
        "        if (chunk_idx + 1) % 5 == 0:\n",
        "            gc.collect()\n",
        "    \n",
        "    print(\"Converting to tensors...\")\n",
        "    # Convert to tensors (from_numpy is faster and shares memory)\n",
        "    X_tensor = torch.from_numpy(X_array).to(torch.float16)\n",
        "    del X_array  # Free numpy array before converting labels\n",
        "    gc.collect()\n",
        "    \n",
        "    # Convert labels to numpy array first (handles pandas Series)\n",
        "    if hasattr(labels, 'values'):\n",
        "        labels = labels.values\n",
        "    y = torch.from_numpy(np.array(labels, dtype=np.int64))\n",
        "    \n",
        "    print(\"Data preparation complete!\")\n",
        "    return X_tensor, y\n",
        "    \n",
        "# # Uses N workers?\n",
        "# def prepare_data_parallel(reviews, labels, word2vec_model, seq_length, n_workers=4, chunk_size=1000):\n",
        "    \"\"\"\n",
        "    Parallel version for even faster processing on multi-core CPUs.\n",
        "    \"\"\"\n",
        "    num_samples = len(reviews)\n",
        "    vector_size = word2vec_model.vector_size\n",
        "    \n",
        "    # Pre-build lookup structures\n",
        "    vocab_set, embedding_matrix, word_to_idx = build_embedding_lookup(word2vec_model)\n",
        "    \n",
        "    # Convert reviews to list if it's a pandas Series\n",
        "    if hasattr(reviews, 'tolist'):\n",
        "        reviews_list = reviews.tolist()\n",
        "    else:\n",
        "        reviews_list = list(reviews)\n",
        "    \n",
        "    # Create partial function with fixed arguments\n",
        "    embed_func = partial(\n",
        "        text_to_embeddings_vectorized,\n",
        "        vocab_set=vocab_set,\n",
        "        embedding_matrix=embedding_matrix,\n",
        "        word_to_idx=word_to_idx,\n",
        "        seq_length=seq_length\n",
        "    )\n",
        "    \n",
        "    # Process in parallel chunks\n",
        "    X_array = np.zeros((num_samples, seq_length, vector_size), dtype=np.float16)\n",
        "    \n",
        "    with Pool(n_workers) as pool:\n",
        "        results = pool.map(embed_func, reviews_list)\n",
        "    \n",
        "    # Fill array from results\n",
        "    for i, embedding in enumerate(results):\n",
        "        X_array[i] = embedding\n",
        "    \n",
        "    # Convert to tensors\n",
        "    X_tensor = torch.from_numpy(X_array).to(torch.float16)\n",
        "    \n",
        "    # Convert labels\n",
        "    if hasattr(labels, 'values'):\n",
        "        labels = labels.values\n",
        "    y = torch.from_numpy(np.array(labels, dtype=np.int64))\n",
        "    \n",
        "    return X_tensor, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1072b94",
      "metadata": {},
      "source": [
        "# Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9864ab6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "DIR_NAME = \"word2vec_tokens_data\"\n",
        "zip_path = f\"{DIR_NAME}.zip\"\n",
        "extract_dir = DIR_NAME\n",
        "\n",
        "# Extract if zip exists but folder doesn't\n",
        "# if os.path.exists(zip_path) and not os.path.exists(extract_dir):\n",
        "#     print(f\"Found {zip_path} - extracting to {extract_dir}...\")\n",
        "#     with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "#         zip_ref.extractall(extract_dir)\n",
        "#     print(\"Extraction complete.\")\n",
        "\n",
        "# If the folder exists, compress its contents into a zip file (no subdirs, just .pkl files and direct children)\n",
        "# if os.path.exists(extract_dir) and not os.path.exists(zip_path):\n",
        "#     print(f\"Zipping files from {extract_dir} into {zip_path} (flat, no folders)...\")\n",
        "#     with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
        "#         for filename in os.listdir(extract_dir):\n",
        "#             file_path = os.path.join(extract_dir, filename)\n",
        "#             if os.path.isfile(file_path):\n",
        "#                 zipf.write(file_path, arcname=filename)\n",
        "#     print(\"Zip file creation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a172ded3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenized data from cache...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenized data loaded from cache!\n",
            "Loading Word2Vec model from cache...\n",
            "Word2Vec model loaded from cache!\n",
            "Vocab size:  116213\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save this code in a separate file named word2vec_tokens.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from gensim.models import Word2Vec\n",
        "import gc  # Added for garbage collection\n",
        "\n",
        "def load_or_create_tokens(train, val, test, preprocess_text, dir_name=DIR_NAME):\n",
        "    \"\"\"\n",
        "    Tokenizes tweet texts and caches the result, or loads from cache if available.\n",
        "    Token files are saved/loaded from '{dir_name}/tokens_train.pkl' etc.\n",
        "    Optimized version using vectorized operations and itertools.\n",
        "    \"\"\"\n",
        "    from collections import Counter\n",
        "    from itertools import chain\n",
        "\n",
        "    train_tokens_path = os.path.join(dir_name, \"tokens_train.pkl\")\n",
        "    val_tokens_path = os.path.join(dir_name, \"tokens_val.pkl\")\n",
        "    test_tokens_path = os.path.join(dir_name, \"tokens_test.pkl\")\n",
        "    valid_tokens_path = os.path.join(dir_name, \"valid_tokens.pkl\")\n",
        "\n",
        "    if os.path.exists(train_tokens_path) and os.path.exists(val_tokens_path) and os.path.exists(test_tokens_path):\n",
        "        print(\"Loading tokenized data from cache...\")\n",
        "        with open(train_tokens_path, \"rb\") as f:\n",
        "            train['tokens'] = pickle.load(f)\n",
        "        with open(val_tokens_path, \"rb\") as f:\n",
        "            val['tokens'] = pickle.load(f)\n",
        "        with open(test_tokens_path, \"rb\") as f:\n",
        "            test['tokens'] = pickle.load(f)\n",
        "        print(\"Tokenized data loaded from cache!\")\n",
        "        gc.collect()  # Run garbage collector after loading from cache\n",
        "    else:\n",
        "        print(\"Tokenizing data (this may take a while)...\")\n",
        "        # Tokenize all datasets\n",
        "        train['tokens'] = train['text of the tweet'].apply(preprocess_text)\n",
        "        val['tokens'] = val['text of the tweet'].apply(preprocess_text)\n",
        "        test['tokens'] = test['text of the tweet'].apply(preprocess_text)\n",
        "        gc.collect()  # Run garbage collector after tokenization\n",
        "\n",
        "        # Count token frequencies in training data (vectorized flattening with itertools)\n",
        "        all_train_tokens = list(chain.from_iterable(train['tokens']))\n",
        "        token_counts = Counter(all_train_tokens)\n",
        "        gc.collect()  # Free memory after counting tokens\n",
        "\n",
        "        # Create set of valid tokens (appearing >= 2 times)\n",
        "        valid_tokens = {token for token, count in token_counts.items() if count >= 2}\n",
        "        # Save token_counts length before deletion for reporting\n",
        "        total_token_count = len(token_counts)\n",
        "        gc.collect()  # Free possibly large token_counts\n",
        "\n",
        "        # Filter tokens using list comprehensions (faster than pandas apply)\n",
        "        # Convert to lists first to avoid pandas overhead during iteration\n",
        "        train_tokens_list = train['tokens'].tolist()\n",
        "        val_tokens_list = val['tokens'].tolist()\n",
        "        test_tokens_list = test['tokens'].tolist()\n",
        "        gc.collect()  # After tolist(), as older token columns may be unreferenced\n",
        "\n",
        "        train['tokens'] = [[t for t in tokens if t in valid_tokens] for tokens in train_tokens_list]\n",
        "        val['tokens'] = [[t for t in tokens if t in valid_tokens] for tokens in val_tokens_list]\n",
        "        test['tokens'] = [[t for t in tokens if t in valid_tokens] for tokens in test_tokens_list]\n",
        "\n",
        "        # Help cleanup large intermediate lists\n",
        "        del train_tokens_list, val_tokens_list, test_tokens_list, all_train_tokens, token_counts\n",
        "        gc.collect()  # Critical garbage collection after largest intermediate data\n",
        "\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        # Save tokenized data\n",
        "        with open(train_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(train['tokens'], f)\n",
        "        with open(val_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(val['tokens'], f)\n",
        "        with open(test_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(test['tokens'], f)\n",
        "        with open(valid_tokens_path, \"wb\") as f:\n",
        "            pickle.dump(valid_tokens, f)\n",
        "        print(f\"Tokenized data saved to cache! Filtered vocabulary: {len(valid_tokens)} tokens (removed {total_token_count - len(valid_tokens)} rare tokens)\")\n",
        "        gc.collect()  # After saving all files, free memory again if needed\n",
        "    return train, val, test\n",
        "\n",
        "def load_or_train_word2vec(train, seq_length, dir_name=DIR_NAME):\n",
        "    \"\"\"\n",
        "    Loads a cached Word2Vec model, or trains and caches a new one.\n",
        "    The model is always saved/loaded as '{dir_name}/word2vec_model.model.'\n",
        "    \"\"\"\n",
        "    model_path = os.path.join(dir_name, \"word2vec_model.model\")\n",
        "    if os.path.exists(model_path):\n",
        "        print(\"Loading Word2Vec model from cache...\")\n",
        "        word2vec_model = Word2Vec.load(model_path)\n",
        "        print(\"Word2Vec model loaded from cache!\")\n",
        "        gc.collect()  # Collect any unreferenced memory after loading\n",
        "    else:\n",
        "        print(\"Training Word2Vec model (this may take a while)...\")\n",
        "        tokens_lists = train['tokens'].values.tolist()\n",
        "        gc.collect()  # Clean up memory before creating model (tokens_lists can be large)\n",
        "        word2vec_model = Word2Vec(\n",
        "            sentences=tokens_lists,\n",
        "            vector_size=seq_length,\n",
        "            min_count=1,\n",
        "            workers=4\n",
        "        )\n",
        "        del tokens_lists  # release reference to input for Word2Vec\n",
        "        gc.collect()  # Collect after model construction\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        word2vec_model.save(model_path)\n",
        "        print(\"Word2Vec model saved to cache!\")\n",
        "        gc.collect()  # Collect after saving model\n",
        "    vocab_size = len(word2vec_model.wv)\n",
        "    print(\"Vocab size: \", vocab_size)\n",
        "    return word2vec_model, vocab_size\n",
        "\n",
        "train, val, test = load_or_create_tokens(train, val, test, preprocess_text)\n",
        "gc.collect()  # One more right after tokens loaded/created\n",
        "\n",
        "seq_length = 100\n",
        "word2vec_model, vocab_size = load_or_train_word2vec(train, seq_length)\n",
        "gc.collect()  # After model loaded/trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec74c468",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocab size: 256064 -->  116213"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "37c352b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading preprocessed training data from cache...\n",
            "Training data loaded from cache!\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "input_size = word2vec_model.vector_size\n",
        "hidden_size = 128\n",
        "output_size = 2 \n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "batch_size = 16\n",
        "dropout_rate = 0.5\n",
        "leaky_relu_slope = 0.1\n",
        "\n",
        "\n",
        "def load_or_prepare_train_data(dir_name, train, word2vec_model, seq_length):\n",
        "    \"\"\"\n",
        "    Loads or creates preprocessed embeddings for training set only.\n",
        "    Returns X_train, y_train tensors.\n",
        "    \"\"\"\n",
        "    X_train_path = os.path.join(dir_name, \"X_train.pt\")\n",
        "    y_train_path = os.path.join(dir_name, \"y_train.pt\")\n",
        "    if os.path.exists(X_train_path) and os.path.exists(y_train_path):\n",
        "        print(\"Loading preprocessed training data from cache...\")\n",
        "        X_train = torch.load(X_train_path, map_location='cpu')\n",
        "        y_train = torch.load(y_train_path, map_location='cpu')\n",
        "        print(\"Training data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing training data (this may take a while)...\")\n",
        "        X_train, y_train = prepare_data(train['tokens'], train['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_train, X_train_path)\n",
        "        torch.save(y_train, y_train_path)\n",
        "        print(\"Training data saved to cache!\")\n",
        "        gc.collect()\n",
        "    return X_train, y_train\n",
        "\n",
        "def load_or_prepare_val_test_data(dir_name, val, test, word2vec_model, seq_length):\n",
        "    \"\"\"\n",
        "    Loads or creates preprocessed embeddings for validation and test sets only.\n",
        "    Returns X_val, y_val, X_test, y_test tensors.\n",
        "    \"\"\"\n",
        "    # VALIDATION\n",
        "    X_val_path = os.path.join(dir_name, \"X_val.pt\")\n",
        "    y_val_path = os.path.join(dir_name, \"y_val.pt\")\n",
        "    if os.path.exists(X_val_path) and os.path.exists(y_val_path):\n",
        "        print(\"Loading preprocessed validation data from cache...\")\n",
        "        X_val = torch.load(X_val_path, map_location='cpu')\n",
        "        y_val = torch.load(y_val_path, map_location='cpu')\n",
        "        print(\"Validation data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing validation data (this may take a while)...\")\n",
        "        X_val, y_val = prepare_data(val['tokens'], val['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_val, X_val_path)\n",
        "        torch.save(y_val, y_val_path)\n",
        "        print(\"Validation data saved to cache!\")\n",
        "        gc.collect()\n",
        "\n",
        "    # TEST\n",
        "    X_test_path = os.path.join(dir_name, \"X_test.pt\")\n",
        "    y_test_path = os.path.join(dir_name, \"y_test.pt\")\n",
        "    if os.path.exists(X_test_path) and os.path.exists(y_test_path):\n",
        "        print(\"Loading preprocessed test data from cache...\")\n",
        "        X_test = torch.load(X_test_path, map_location='cpu')\n",
        "        y_test = torch.load(y_test_path, map_location='cpu')\n",
        "        print(\"Test data loaded from cache!\")\n",
        "    else:\n",
        "        print(\"Preprocessing test data (this may take a while)...\")\n",
        "        X_test, y_test = prepare_data(test['tokens'], test['label'],\n",
        "                            word2vec_model, seq_length=seq_length)\n",
        "        os.makedirs(dir_name, exist_ok=True)\n",
        "        torch.save(X_test, X_test_path)\n",
        "        torch.save(y_test, y_test_path)\n",
        "        print(\"Test data saved to cache!\")\n",
        "        gc.collect()\n",
        "    \n",
        "    return X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# Prepare training data - load or create embeddings\n",
        "X_train, y_train = load_or_prepare_train_data(\n",
        "    DIR_NAME, train, word2vec_model, seq_length\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b925ad9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing validation data (this may take a while)...\n",
            "Processing 287999 samples with chunk_size=500...\n",
            "Building embedding lookup structures...\n",
            "Lookup structures built. Vocab size: 116213\n",
            "Allocating output array: 287999 x 100 x 100 (float16)...\n",
            "Processing in 576 chunks...\n",
            "Processing chunk 1/576 (samples 0-500)...\n",
            "Processing chunk 10/576 (samples 4500-5000)...\n",
            "Processing chunk 20/576 (samples 9500-10000)...\n",
            "Processing chunk 30/576 (samples 14500-15000)...\n",
            "Processing chunk 40/576 (samples 19500-20000)...\n",
            "Processing chunk 50/576 (samples 24500-25000)...\n",
            "Processing chunk 60/576 (samples 29500-30000)...\n",
            "Processing chunk 70/576 (samples 34500-35000)...\n",
            "Processing chunk 80/576 (samples 39500-40000)...\n",
            "Processing chunk 90/576 (samples 44500-45000)...\n",
            "Processing chunk 100/576 (samples 49500-50000)...\n",
            "Processing chunk 110/576 (samples 54500-55000)...\n",
            "Processing chunk 120/576 (samples 59500-60000)...\n",
            "Processing chunk 130/576 (samples 64500-65000)...\n",
            "Processing chunk 140/576 (samples 69500-70000)...\n",
            "Processing chunk 150/576 (samples 74500-75000)...\n",
            "Processing chunk 160/576 (samples 79500-80000)...\n",
            "Processing chunk 170/576 (samples 84500-85000)...\n",
            "Processing chunk 180/576 (samples 89500-90000)...\n",
            "Processing chunk 190/576 (samples 94500-95000)...\n",
            "Processing chunk 200/576 (samples 99500-100000)...\n",
            "Processing chunk 210/576 (samples 104500-105000)...\n",
            "Processing chunk 220/576 (samples 109500-110000)...\n",
            "Processing chunk 230/576 (samples 114500-115000)...\n",
            "Processing chunk 240/576 (samples 119500-120000)...\n",
            "Processing chunk 250/576 (samples 124500-125000)...\n",
            "Processing chunk 260/576 (samples 129500-130000)...\n",
            "Processing chunk 270/576 (samples 134500-135000)...\n",
            "Processing chunk 280/576 (samples 139500-140000)...\n",
            "Processing chunk 290/576 (samples 144500-145000)...\n",
            "Processing chunk 300/576 (samples 149500-150000)...\n",
            "Processing chunk 310/576 (samples 154500-155000)...\n",
            "Processing chunk 320/576 (samples 159500-160000)...\n",
            "Processing chunk 330/576 (samples 164500-165000)...\n",
            "Processing chunk 340/576 (samples 169500-170000)...\n",
            "Processing chunk 350/576 (samples 174500-175000)...\n",
            "Processing chunk 360/576 (samples 179500-180000)...\n",
            "Processing chunk 370/576 (samples 184500-185000)...\n",
            "Processing chunk 380/576 (samples 189500-190000)...\n",
            "Processing chunk 390/576 (samples 194500-195000)...\n",
            "Processing chunk 400/576 (samples 199500-200000)...\n",
            "Processing chunk 410/576 (samples 204500-205000)...\n",
            "Processing chunk 420/576 (samples 209500-210000)...\n",
            "Processing chunk 430/576 (samples 214500-215000)...\n",
            "Processing chunk 440/576 (samples 219500-220000)...\n",
            "Processing chunk 450/576 (samples 224500-225000)...\n",
            "Processing chunk 460/576 (samples 229500-230000)...\n",
            "Processing chunk 470/576 (samples 234500-235000)...\n"
          ]
        }
      ],
      "source": [
        "# Prepare validation and test data - load or create embeddings\n",
        "X_val, y_val, X_test, y_test = load_or_prepare_val_test_data(\n",
        "    DIR_NAME, val, test, word2vec_model, seq_length\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9192ac0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataLoader\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150220c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "val_data = TensorDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f632f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "test_data = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "gc.collect() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48cdd93f",
      "metadata": {},
      "source": [
        "# Define the RNN model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9622ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "# class SentimentRNN(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope=0.1):\n",
        "#         super(SentimentRNN, self).__init__()\n",
        "#         self.hidden_size = hidden_size\n",
        "#         self.num_layers = num_layers\n",
        "#         # Basic RNN layer\n",
        "#         # shape of input tensor: (batch_size, seq_length, input_size)\n",
        "#         self.rnn = nn.RNN(\n",
        "#             input_size,\n",
        "#             hidden_size,\n",
        "#             num_layers,\n",
        "#             batch_first=True,\n",
        "#             dropout=dropout_rate if num_layers > 1 else 0,\n",
        "#         )\n",
        "#         self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
        "#         self.dropout = nn.Dropout(dropout_rate)\n",
        "#         self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Initial hidden state\n",
        "#         # h0 shape: (num_layers, batch_size, hidden_size)\n",
        "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
        "\n",
        "#         # out shape after rnn: (batch_size, seq_length, hidden_size)\n",
        "#         out, _ = self.rnn(x, h0)\n",
        "#         out = self.leaky_relu(out)\n",
        "#         out = self.dropout(out)\n",
        "\n",
        "#         # Get the last sequence output for classification\n",
        "#         # out shape after indexing: (batch_size, hidden_size)\n",
        "#         out = out[:, -1, :]\n",
        "\n",
        "#         logits = self.fc(out)\n",
        "#         return logits\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope=0.1):\n",
        "        super(SentimentRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        # Basic RNN layer\n",
        "        # shape of input tensor: (batch_size, seq_length, input_size)\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout_rate if num_layers > 1 else 0,\n",
        "        )\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial hidden state - convert to float16\n",
        "        # h0 shape: (num_layers, batch_size, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, \n",
        "                        dtype=torch.float16, device=x.device)\n",
        "\n",
        "        # out shape after rnn: (batch_size, seq_length, hidden_size)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.leaky_relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Get the last sequence output for classification\n",
        "        # out shape after indexing: (batch_size, hidden_size)\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "        \n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SentimentRNN(\n",
        "    input_size,\n",
        "    hidden_size,\n",
        "    output_size,\n",
        "    num_layers,\n",
        "    dropout_rate,\n",
        "    leaky_relu_slope=leaky_relu_slope,\n",
        ").to(device)\n",
        "\n",
        "# Convert model to float16\n",
        "model = model.half()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Number of trainable parameters: {num_trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb65eef",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9ce77d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "num_epochs = 30  # Number of epochs\n",
        "losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_epoch = 0\n",
        "patience = 0\n",
        "max_patience = 3\n",
        "\n",
        "# Start timing training\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"EPOCH  NUMBER: {epoch+1}\")\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Convert inputs to float16\n",
        "        inputs = inputs.to(device, dtype=torch.float16)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # CrossEntropyLoss expects long integer class indices (0 or 1)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)  # Shape: (batch_size, 2)\n",
        "        loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "        count += 1\n",
        "\n",
        "    average_loss = total_loss / max(count, 1)\n",
        "    losses.append(average_loss)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    val_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            # Convert inputs to float16\n",
        "            inputs = inputs.to(device, dtype=torch.float16)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(inputs)  # Shape: (batch_size, 2)\n",
        "            val_loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
        "\n",
        "            total_val_loss += float(val_loss.item())\n",
        "            val_count += 1\n",
        "\n",
        "    average_val_loss = total_val_loss / max(val_count, 1)\n",
        "    val_losses.append(average_val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
        "\n",
        "    # Save best checkpoint\n",
        "    if average_val_loss < best_val_loss:\n",
        "        best_val_loss = average_val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"rnn_best_model.pth\")\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience += 1\n",
        "\n",
        "    if patience >= max_patience:\n",
        "        print(f\"Early stopped at {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Calculate total training time\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}\")\n",
        "print(f\"Total Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c1bf43",
      "metadata": {},
      "source": [
        "# Evaluate "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b258fa8",
      "metadata": {},
      "source": [
        "## Evaluation Functions - Accuracy, Label Precision, Recall, F1, and Confusion Matric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b16bac",
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics import evaluate_model\n",
        "\n",
        "# Calculate number of trainable parameters\n",
        "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f72d494",
      "metadata": {},
      "source": [
        "## Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "407117aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model and calculate accuracy only for that\n",
        "model.load_state_dict(torch.load(\"rnn_best_model.pth\", map_location=device))\n",
        "model.to(device)\n",
        "\n",
        "# After training, evaluate on validation set (with confusion matrix plot)\n",
        "val_metrics = evaluate_model(\n",
        "    model, \n",
        "    device, \n",
        "    val_loader,\n",
        "    label_0,\n",
        "    label_4,\n",
        "    plot_confusion_matrix=True,\n",
        "    title=\"Validation Set Confusion Matrix\",\n",
        "    training_time_seconds=training_time,\n",
        "    num_trainable_parameters=num_trainable_params,\n",
        "    model_name=\"rnn\",\n",
        "    dataset_split=\"val\",\n",
        "    save_results=True,\n",
        ")\n",
        "\n",
        "print(f\"Validation Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
        "print(f\"Validation Metrics: {val_metrics}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "219bab26",
      "metadata": {},
      "source": [
        "## Test Set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad0b959",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set (without plot)\n",
        "\n",
        "# Or plot test confusion matrix separately\n",
        "test_metrics = evaluate_model(\n",
        "    model, \n",
        "    device, \n",
        "    test_loader,\n",
        "    label_0,\n",
        "    label_4,\n",
        "    plot_confusion_matrix=True,\n",
        "    title=\"Test Set Confusion Matrix\",\n",
        "    training_time_seconds=training_time,\n",
        "     num_trainable_parameters=num_trainable_params,\n",
        "    model_name=\"rnn\",\n",
        "    dataset_split=\"test\",\n",
        "    save_results=True,\n",
        ")\n",
        "\n",
        "print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
