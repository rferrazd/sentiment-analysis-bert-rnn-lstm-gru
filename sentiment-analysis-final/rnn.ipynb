{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82de422e",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "*INSTRUCTIONS*\n",
    "Embedding for RNN-based Models:\n",
    "    ○ Generate wordembeddings using GloVeorWord2Vec.\n",
    "    ○ Pad sequences to a fixed length for uniformity\n",
    "\n",
    "Steps were taken from notebook: Module 3 - Video 6 onwards.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ace5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check the available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the datasets\n",
    "with open(\"data/train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"data/val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"data/test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "# Remap original labels {0,4} -> contiguous ids {0,1} for modeling\n",
    "# (Keep a copy of the original labels for reporting/debugging.)\n",
    "LABEL_MAP = {0: 0, 4: 1}\n",
    "\n",
    "for df_name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "    df[\"label_original\"] = df[\"label\"]\n",
    "    mapped = df[\"label_original\"].map(LABEL_MAP)\n",
    "\n",
    "    if mapped.isna().any():\n",
    "        bad = sorted(df.loc[mapped.isna(), \"label_original\"].unique().tolist())\n",
    "        raise ValueError(f\"Unexpected labels in {df_name}: {bad}\")\n",
    "\n",
    "    df[\"label\"] = mapped.astype(int)\n",
    "\n",
    "num_labels = train[\"label\"].nunique()\n",
    "print(\"Number of labels (label): \", num_labels)\n",
    "\n",
    "# With remapping, class ids are stable\n",
    "label_0 = 0  # Negative\n",
    "label_4 = 1  # Positive\n",
    "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec17c5",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d122a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utils\n",
    "# ---------------------------------------\n",
    "\n",
    "# Function to convert text to tokens\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Keep only letters and whitespaces (avoid regex escape warnings)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function to convert tokens to Word2Vec embeddings\n",
    "def text_to_embeddings(text, word2vec_model, seq_length):\n",
    "    \"\"\"\n",
    "    Function to convert a given token list into a sequence of embeddings using a pretrained Word2Vec model.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i, word in enumerate(text):\n",
    "        if i == seq_length:\n",
    "            break\n",
    "        if word in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[word])\n",
    "        # else: skip OOV tokens\n",
    "\n",
    "    # Padding the sequences\n",
    "    if len(embeddings) < seq_length:\n",
    "        zero_padding = [np.zeros(word2vec_model.vector_size, dtype=np.float32)\n",
    "                        for _ in range(seq_length - len(embeddings))]\n",
    "        embeddings = embeddings + zero_padding\n",
    "\n",
    "    # Sequence of word vectors of length seq_length\n",
    "    return embeddings[:seq_length]\n",
    "\n",
    "# Text -> Embeddings -> torch tensors\n",
    "def prepare_data(reviews, labels, word2vec_model, seq_length):\n",
    "    X = [text_to_embeddings(review, word2vec_model, seq_length) for review in reviews]\n",
    "    X_array = np.array(X, dtype=np.float32)\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return X_tensor, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1072b94",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Tokenize the tweet reviwes\n",
    "train['tokens'] = train['text of the tweet'].apply(preprocess_text)\n",
    "val['tokens'] = val['text of the tweet'].apply(preprocess_text)\n",
    "test['tokens'] = test['text of the tweet'].apply(preprocess_text)\n",
    "\n",
    "seq_length = 100\n",
    "# 2) Create vocabulary using word2vec\n",
    "word2vec_model = Word2Vec(sentences=train['tokens'].values.tolist(), \n",
    "                          vector_size=seq_length, # same as in Module 3 - Video 6 onwards.ipynb\n",
    "                           min_count=1, \n",
    "                           workers=4)\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(word2vec_model.wv)\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c352b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = word2vec_model.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 2 \n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "dropout_rate = 0.5\n",
    "leaky_relu_slope = 0.1\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = prepare_data(train['tokens'], train['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a957d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_data(val['tokens'], val['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prepare_data(test['tokens'], test['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdd93f",
   "metadata": {},
   "source": [
    "# Define the RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9622ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope=0.1):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Basic RNN layer\n",
    "        # shape of input tensor: (batch_size, seq_length, input_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden state\n",
    "        # h0 shape: (num_layers, batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "\n",
    "        # out shape after rnn: (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Get the last sequence output for classification\n",
    "        # out shape after indexing: (batch_size, hidden_size)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SentimentRNN(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dropout_rate,\n",
    "    leaky_relu_slope=leaky_relu_slope,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb65eef",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "num_epochs = 30  # Number of epochs\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "patience = 0\n",
    "max_patience = 3\n",
    "\n",
    "# Start timing training\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # CrossEntropyLoss expects long integer class indices (0 or 1)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)  # Shape: (batch_size, 2)\n",
    "        loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        count += 1\n",
    "\n",
    "    average_loss = total_loss / max(count, 1)\n",
    "    losses.append(average_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "    \n",
    "            logits = model(inputs)  # Shape: (batch_size, 2)\n",
    "            val_loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
    "\n",
    "            total_val_loss += float(val_loss.item())\n",
    "            val_count += 1\n",
    "\n",
    "    average_val_loss = total_val_loss / max(val_count, 1)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"rnn_best_model.pth\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f\"Early stopped at {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Calculate total training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}\")\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1bf43",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b258fa8",
   "metadata": {},
   "source": [
    "## Evaluation Functions - Accuracy, Label Precision, Recall, F1, and Confusion Matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b16bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_model\n",
    "\n",
    "# Calculate number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72d494",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407117aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and calculate accuracy only for that\n",
    "model.load_state_dict(torch.load(\"rnn_best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# After training, evaluate on validation set (with confusion matrix plot)\n",
    "val_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    val_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Validation Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "    num_trainable_parameters=num_trainable_params,\n",
    "    model_name=\"rnn\",\n",
    "    dataset_split=\"val\",\n",
    "    save_results=True,\n",
    ")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
    "print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bab26",
   "metadata": {},
   "source": [
    "## Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (without plot)\n",
    "\n",
    "# Or plot test confusion matrix separately\n",
    "test_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    test_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Test Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "     num_trainable_parameters=num_trainable_params,\n",
    "    model_name=\"rnn\",\n",
    "    dataset_split=\"test\",\n",
    "    save_results=True,\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
