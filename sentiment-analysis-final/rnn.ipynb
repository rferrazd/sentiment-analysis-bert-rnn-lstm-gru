{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82de422e",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "*INSTRUCTIONS*\n",
    "Embedding for RNN-based Models:\n",
    "    ○ Generate wordembeddings using GloVeorWord2Vec.\n",
    "    ○ Pad sequences to a fixed length for uniformity\n",
    "\n",
    "Steps were taken from notebook: Module 3 - Video 6 onwards.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5ace5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Number of labels (label):  2\n",
      "Label 0: 0 and label 4: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweet</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>label_original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237034</th>\n",
       "      <td>0</td>\n",
       "      <td>2058468667</td>\n",
       "      <td>Sat Jun 06 15:00:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bestthingaround</td>\n",
       "      <td>my star trek bootleg timed out and when i refr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387008</th>\n",
       "      <td>0</td>\n",
       "      <td>2068651245</td>\n",
       "      <td>Sun Jun 07 14:27:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Scriblit</td>\n",
       "      <td>yeah but the really pretty ones only go up to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
       "237034                   0       2058468667  Sat Jun 06 15:00:18 PDT 2009   \n",
       "1387008                  0       2068651245  Sun Jun 07 14:27:20 PDT 2009   \n",
       "\n",
       "            query             user  \\\n",
       "237034   NO_QUERY  bestthingaround   \n",
       "1387008  NO_QUERY         Scriblit   \n",
       "\n",
       "                                         text of the tweet  label  \\\n",
       "237034   my star trek bootleg timed out and when i refr...      0   \n",
       "1387008  yeah but the really pretty ones only go up to ...      0   \n",
       "\n",
       "         label_original  \n",
       "237034                0  \n",
       "1387008               0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check the available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the datasets\n",
    "with open(\"data/train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"data/val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"data/test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "# Remap original labels {0,4} -> contiguous ids {0,1} for modeling\n",
    "# (Keep a copy of the original labels for reporting/debugging.)\n",
    "LABEL_MAP = {0: 0, 4: 1}\n",
    "\n",
    "for df_name, df in [(\"train\", train), (\"val\", val), (\"test\", test)]:\n",
    "    df[\"label_original\"] = df[\"label\"]\n",
    "    mapped = df[\"label_original\"].map(LABEL_MAP)\n",
    "\n",
    "    if mapped.isna().any():\n",
    "        bad = sorted(df.loc[mapped.isna(), \"label_original\"].unique().tolist())\n",
    "        raise ValueError(f\"Unexpected labels in {df_name}: {bad}\")\n",
    "\n",
    "    df[\"label\"] = mapped.astype(int)\n",
    "\n",
    "num_labels = train[\"label\"].nunique()\n",
    "print(\"Number of labels (label): \", num_labels)\n",
    "\n",
    "# With remapping, class ids are stable\n",
    "label_0 = 0  # Negative\n",
    "label_4 = 1  # Positive\n",
    "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec17c5",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d122a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Utils\n",
    "# ---------------------------------------\n",
    "\n",
    "# Function to convert text to tokens\n",
    "def preprocess_text(text):\n",
    "    # Check if the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    # Keep only letters and whitespaces (avoid regex escape warnings)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Function to convert tokens to Word2Vec embeddings\n",
    "def text_to_embeddings(text, word2vec_model, seq_length):\n",
    "    \"\"\"\n",
    "    Function to convert a given token list into a sequence of embeddings using a pretrained Word2Vec model.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "\n",
    "    for i, word in enumerate(text):\n",
    "        if i == seq_length:\n",
    "            break\n",
    "        if word in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[word])\n",
    "        # else: skip OOV tokens\n",
    "\n",
    "    # Padding the sequences\n",
    "    if len(embeddings) < seq_length:\n",
    "        zero_padding = [np.zeros(word2vec_model.vector_size, dtype=np.float32)\n",
    "                        for _ in range(seq_length - len(embeddings))]\n",
    "        embeddings = embeddings + zero_padding\n",
    "\n",
    "    # Sequence of word vectors of length seq_length\n",
    "    return embeddings[:seq_length]\n",
    "\n",
    "# Text -> Embeddings -> torch tensors\n",
    "def prepare_data(reviews, labels, word2vec_model, seq_length):\n",
    "    X = [text_to_embeddings(review, word2vec_model, seq_length) for review in reviews]\n",
    "    X_array = np.array(X, dtype=np.float32)\n",
    "    X_tensor = torch.tensor(X_array, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.long)\n",
    "    return X_tensor, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1072b94",
   "metadata": {},
   "source": [
    "# Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a172ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  256064\n"
     ]
    }
   ],
   "source": [
    "# 1) Tokenize the tweet reviwes\n",
    "train['tokens'] = train['text of the tweet'].apply(preprocess_text)\n",
    "val['tokens'] = val['text of the tweet'].apply(preprocess_text)\n",
    "test['tokens'] = test['text of the tweet'].apply(preprocess_text)\n",
    "\n",
    "seq_length = 100\n",
    "# 2) Create vocabulary using word2vec\n",
    "word2vec_model = Word2Vec(sentences=train['tokens'].values.tolist(), \n",
    "                          vector_size=seq_length, # same as in Module 3 - Video 6 onwards.ipynb\n",
    "                           min_count=1, \n",
    "                           workers=4)\n",
    "\n",
    "# Get vocabulary size\n",
    "vocab_size = len(word2vec_model.wv)\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c352b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "input_size = word2vec_model.vector_size\n",
    "hidden_size = 128\n",
    "output_size = 2 \n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "dropout_rate = 0.5\n",
    "leaky_relu_slope = 0.1\n",
    "\n",
    "# Prepare data\n",
    "X_train, y_train = prepare_data(train['tokens'], train['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a957d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_data(val['tokens'], val['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prepare_data(test['tokens'], test['label'],\n",
    "                    word2vec_model, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader\n",
    "val_data = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cdd93f",
   "metadata": {},
   "source": [
    "# Define the RNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9622ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate, leaky_relu_slope=0.1):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Basic RNN layer\n",
    "        # shape of input tensor: (batch_size, seq_length, input_size)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=leaky_relu_slope)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial hidden state\n",
    "        # h0 shape: (num_layers, batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=x.device)\n",
    "\n",
    "        # out shape after rnn: (batch_size, seq_length, hidden_size)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.leaky_relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Get the last sequence output for classification\n",
    "        # out shape after indexing: (batch_size, hidden_size)\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SentimentRNN(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dropout_rate,\n",
    "    leaky_relu_slope=leaky_relu_slope,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb65eef",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ce77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "num_epochs = 30  # Number of epochs\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_epoch = 0\n",
    "patience = 0\n",
    "max_patience = 3\n",
    "\n",
    "# Start timing training\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # CrossEntropyLoss expects long integer class indices (0 or 1)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)  # Shape: (batch_size, 2)\n",
    "        loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        count += 1\n",
    "\n",
    "    average_loss = total_loss / max(count, 1)\n",
    "    losses.append(average_loss)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "    \n",
    "            logits = model(inputs)  # Shape: (batch_size, 2)\n",
    "            val_loss = criterion(logits, labels)  # labels shape: (batch_size) with class indices\n",
    "\n",
    "            total_val_loss += float(val_loss.item())\n",
    "            val_count += 1\n",
    "\n",
    "    average_val_loss = total_val_loss / max(val_count, 1)\n",
    "    val_losses.append(average_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {average_loss:.4f}, Val Loss: {average_val_loss:.4f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"rnn_best_model.pth\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if patience >= max_patience:\n",
    "        print(f\"Early stopped at {epoch+1}\")\n",
    "        break\n",
    "\n",
    "# Calculate total training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Lowest Validation Loss: {best_val_loss:.4f} at Epoch {best_epoch + 1}\")\n",
    "print(f\"Total Training Time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c1bf43",
   "metadata": {},
   "source": [
    "# Evaluate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b258fa8",
   "metadata": {},
   "source": [
    "## Evaluation Functions - Accuracy, Label Precision, Recall, F1, and Confusion Matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b16bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72d494",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407117aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model and calculate accuracy only for that\n",
    "model.load_state_dict(torch.load(\"rnn_best_model.pth\", map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# After training, evaluate on validation set (with confusion matrix plot)\n",
    "val_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    val_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Validation Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "    model_name=\"rnn\",\n",
    "    dataset_split=\"val\",\n",
    "    save_results=True,\n",
    ")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_metrics['accuracy']:.2f}%\")\n",
    "print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bab26",
   "metadata": {},
   "source": [
    "## Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (without plot)\n",
    "test_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    test_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=False,\n",
    "    training_time_seconds=training_time,\n",
    "    model_name=\"rnn\",\n",
    "    dataset_split=\"test\",\n",
    "    save_results=True,\n",
    ")\n",
    "print(f\"Test Accuracy: {test_metrics['accuracy']:.2f}%\")\n",
    "\n",
    "# Or plot test confusion matrix separately\n",
    "test_metrics = evaluate_model(\n",
    "    model, \n",
    "    device, \n",
    "    test_loader,\n",
    "    label_0,\n",
    "    label_4,\n",
    "    plot_confusion_matrix=True,\n",
    "    title=\"Test Set Confusion Matrix\",\n",
    "    training_time_seconds=training_time,\n",
    "    model_name=\"rnn\",\n",
    "    dataset_split=\"test\",\n",
    "    save_results=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
