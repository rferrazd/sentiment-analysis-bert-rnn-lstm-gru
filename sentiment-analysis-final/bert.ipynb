{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91223976",
   "metadata": {},
   "source": [
    "# BERT\n",
    "Tokenizer: WordPiece\n",
    "\n",
    "EvaluationMetrics\n",
    "    ● Accuracy:Overall percentage of correct predictions.\n",
    "    ● Precision,Recall,F1-Score:Evaluate per class(negative,neutral, positive).\n",
    "    ● Confusion Matrix:Show performance across all classes.\n",
    "    ● ROC-AUCScore: Measure the ability of the model to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweet</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237034</th>\n",
       "      <td>0</td>\n",
       "      <td>2058468667</td>\n",
       "      <td>Sat Jun 06 15:00:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bestthingaround</td>\n",
       "      <td>my star trek bootleg timed out and when i refr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387008</th>\n",
       "      <td>0</td>\n",
       "      <td>2068651245</td>\n",
       "      <td>Sun Jun 07 14:27:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Scriblit</td>\n",
       "      <td>yeah but the really pretty ones only go up to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
       "237034                   0       2058468667  Sat Jun 06 15:00:18 PDT 2009   \n",
       "1387008                  0       2068651245  Sun Jun 07 14:27:20 PDT 2009   \n",
       "\n",
       "            query             user  \\\n",
       "237034   NO_QUERY  bestthingaround   \n",
       "1387008  NO_QUERY         Scriblit   \n",
       "\n",
       "                                         text of the tweet  label  \n",
       "237034   my star trek bootleg timed out and when i refr...      0  \n",
       "1387008  yeah but the really pretty ones only go up to ...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check the available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "# Load the datasets \n",
    "with open(\"train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "num_labels = train['label'].nunique()\n",
    "print(\"Number of labels: \", num_labels)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c957a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length accepted by the tokenizer: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertTokenizer\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels  \n",
    ").to(device)\n",
    "\n",
    "# To check the maximum sequence length accepted by the tokenizer/model\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "print(f\"Max sequence length accepted by the tokenizer: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6c740",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a39395",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb5dcf",
   "metadata": {},
   "source": [
    "# Build a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "858ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.iloc[idx]['text of the tweet'])\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            #  return_tensors='pt', these have shape [1, max_length]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': int(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb91dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([512])\n",
      "Batched input_ids shape: torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = CustomDataset(train, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "val_dataset = CustomDataset(val, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "test_dataset = CustomDataset(test, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check the shape of a single item\n",
    "sample = train_dataset[0]\n",
    "print(f\"input_ids shape: {sample['input_ids'].shape}\")  # Without flatten: [1, 512]\n",
    "\n",
    "# Check the shape after batching\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batched input_ids shape: {batch['input_ids'].shape}\")  # Should be [16, 512], not [16, 1, 512]\n",
    "# BERT expects bs,max_seq_length\n",
    "\n",
    "# ========\n",
    "# SET UP\n",
    "# ========\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ===========\n",
    "# METRICS\n",
    "# ===========\n",
    "\n",
    "# Precision: TP / (TP + FP)\n",
    "# Recall:  TP / (TP + FN)\n",
    "def calculate_precision_recall_f1(model, loader, device, label_0, label_4):\n",
    "    \"\"\"\n",
    "    Calculate precision and recall for two labels (label_0, label_4) in a classification task.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Initialize counts\n",
    "    tp_0, fp_0, fn_0 = 0, 0, 0\n",
    "    tp_4, fp_4, fn_4 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            # For label_0\n",
    "            tp_0 += ((predictions == label_0) & (labels == label_0)).sum().item()\n",
    "            fp_0 += ((predictions == label_0) & (labels != label_0)).sum().item()\n",
    "            fn_0 += ((predictions != label_0) & (labels == label_0)).sum().item()\n",
    "            # For label_4\n",
    "            tp_4 += ((predictions == label_4) & (labels == label_4)).sum().item()\n",
    "            fp_4 += ((predictions == label_4) & (labels != label_4)).sum().item()\n",
    "            fn_4 += ((predictions != label_4) & (labels == label_4)).sum().item()\n",
    "\n",
    "    precision_0 = tp_0 / (tp_0 + fp_0) if (tp_0 + fp_0) > 0 else 0.0\n",
    "    recall_0 = tp_0 / (tp_0 + fn_0) if (tp_0 + fn_0) > 0 else 0.0\n",
    "    precision_4 = tp_4 / (tp_4 + fp_4) if (tp_4 + fp_4) > 0 else 0.0\n",
    "    recall_4 = tp_4 / (tp_4 + fn_4) if (tp_4 + fn_4) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'precision_label_0': precision_0, \n",
    "        'recall_label_0': recall_0,\n",
    "        'f1_label_0' : 2*(precision_0*recall_0) / (precision_0+recall_0) if (precision_0+recall_0) > 0 else 0.0,\n",
    "        'precision_label_4': precision_4, \n",
    "        'f1_label_4' : 2*(precision_4*recall_4) / (precision_4+recall_4) if (precision_4+recall_4) > 0 else 0.0,\n",
    "        'recall_label_4': recall_4\n",
    "    }\n",
    "\n",
    "# Accuracy\n",
    "def calculate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Confusion Matrix\n",
    "def get_confusion_matrix(model, loader, device):\n",
    "    \"\"\"\n",
    "    Get predictions and true labels for confusion matrix calculation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_labels), np.array(all_predictions)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix heatmap.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84cab410",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for i, label in enumerate(train_dataset.data['label'].value_counts().index)]\n",
    "labels\n",
    "label_0 = labels[1]\n",
    "label_4 = labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12ceae",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')  # Initialize best_val_loss to a very high value\n",
    "best_epoch = -1  # Initialize best_epoch to an invalid value to track the epoch of the best validation loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training with progress bar\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS} [Train]', leave=False)\n",
    "    for batch in train_pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation with progress bar\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{EPOCHS} [Val]', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Check if the current validation loss is the lowest; if so, save the model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Print the best epoch and its validation loss\n",
    "print(f\"The lowest validation loss was {best_val_loss:.4f} at epoch {best_epoch + 1}\")\n",
    "\n",
    "# Load the best model before calculating final metrics\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "train_precision = calculate_precision_recall_f1(model, train_loader, device, label_0, label_4)\n",
    "val_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "val_precision = calculate_precision_recall_f1(model, val_loader, device, label_0, label_4)\n",
    "print(f'Best Model Training Accuracy: {train_accuracy:.2f}%')\n",
    "print(f'Best Model Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Print precision, recall, and F1 scores\n",
    "print(f'\\nTraining Metrics:')\n",
    "print(f\"  Label {label_0} - Precision: {train_precision['precision_label_0']:.4f}, Recall: {train_precision['recall_label_0']:.4f}, F1: {train_precision['f1_label_0']:.4f}\")\n",
    "print(f\"  Label {label_4} - Precision: {train_precision['precision_label_4']:.4f}, Recall: {train_precision['recall_label_4']:.4f}, F1: {train_precision['f1_label_4']:.4f}\")\n",
    "\n",
    "print(f'\\nValidation Metrics:')\n",
    "print(f\"  Label {label_0} - Precision: {val_precision['precision_label_0']:.4f}, Recall: {val_precision['recall_label_0']:.4f}, F1: {val_precision['f1_label_0']:.4f}\")\n",
    "print(f\"  Label {label_4} - Precision: {val_precision['precision_label_4']:.4f}, Recall: {val_precision['recall_label_4']:.4f}, F1: {val_precision['f1_label_4']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2251474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrices\n",
    "# Map labels to class names (0 = Negative, 4 = Positive based on your assignment)\n",
    "label_to_name = {0: 'Negative', 4: 'Positive'}\n",
    "unique_labels = sorted(train['label'].unique())\n",
    "class_names = [label_to_name.get(label, f'Label {label}') for label in unique_labels]\n",
    "\n",
    "# Confusion Matrix for Validation Set\n",
    "print(\"Validation Set Confusion Matrix:\")\n",
    "y_true_val, y_pred_val = get_confusion_matrix(model, val_loader, device)\n",
    "cm_val = plot_confusion_matrix(y_true_val, y_pred_val, class_names, \n",
    "                                title=\"Validation Set - Confusion Matrix\")\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "y_true_test, y_pred_test = get_confusion_matrix(model, test_loader, device)\n",
    "cm_test = plot_confusion_matrix(y_true_test, y_pred_test, class_names, \n",
    "                                 title=\"Test Set - Confusion Matrix\")\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f'\\nTest Set Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
