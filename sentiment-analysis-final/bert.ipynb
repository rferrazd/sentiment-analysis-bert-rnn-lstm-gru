{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91223976",
   "metadata": {},
   "source": [
    "# BERT\n",
    "Tokenizer: WordPiece\n",
    "\n",
    "EvaluationMetrics\n",
    "    ● Accuracy:Overall percentage of correct predictions.\n",
    "    ● Precision,Recall,F1-Score:Evaluate per class(negative,neutral, positive).\n",
    "    ● Confusion Matrix:Show performance across all classes.\n",
    "    ● ROC-AUCScore: Measure the ability of the model to distinguish between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779ea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Number of labels:  2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity of tweet</th>\n",
       "      <th>id of the tweet</th>\n",
       "      <th>date of the tweet</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text of the tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237034</th>\n",
       "      <td>0</td>\n",
       "      <td>2058468667</td>\n",
       "      <td>Sat Jun 06 15:00:18 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bestthingaround</td>\n",
       "      <td>my star trek bootleg timed out and when i refr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387008</th>\n",
       "      <td>0</td>\n",
       "      <td>2068651245</td>\n",
       "      <td>Sun Jun 07 14:27:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Scriblit</td>\n",
       "      <td>yeah but the really pretty ones only go up to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity of tweet  id of the tweet             date of the tweet  \\\n",
       "237034                   0       2058468667  Sat Jun 06 15:00:18 PDT 2009   \n",
       "1387008                  0       2068651245  Sun Jun 07 14:27:20 PDT 2009   \n",
       "\n",
       "            query             user  \\\n",
       "237034   NO_QUERY  bestthingaround   \n",
       "1387008  NO_QUERY         Scriblit   \n",
       "\n",
       "                                         text of the tweet  label  \n",
       "237034   my star trek bootleg timed out and when i refr...      0  \n",
       "1387008  yeah but the really pretty ones only go up to ...      0  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "# local imports\n",
    "from metrics import calculate_precision_recall_f1, calculate_accuracy, get_confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check the available device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "# Load the datasets \n",
    "with open(\"train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "num_labels = train['label'].nunique()\n",
    "print(\"Number of labels: \", num_labels)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24c957a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length accepted by the tokenizer: 512\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, BertTokenizer\n",
    "\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels  \n",
    ").to(device)\n",
    "\n",
    "# To check the maximum sequence length accepted by the tokenizer/model\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "print(f\"Max sequence length accepted by the tokenizer: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6c740",
   "metadata": {},
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d8a39395",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb5dcf",
   "metadata": {},
   "source": [
    "# Build a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "858ce9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_seq_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.data.iloc[idx]['text of the tweet'])\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_seq_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            #  return_tensors='pt', these have shape [1, max_length]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': int(label)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb91dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([512])\n",
      "Batched input_ids shape: torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = CustomDataset(train, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "val_dataset = CustomDataset(val, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "test_dataset = CustomDataset(test, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check the shape of a single item\n",
    "sample = train_dataset[0]\n",
    "print(f\"input_ids shape: {sample['input_ids'].shape}\")  # Without flatten: [1, 512]\n",
    "\n",
    "# Check the shape after batching\n",
    "batch = next(iter(train_loader))\n",
    "print(f\"Batched input_ids shape: {batch['input_ids'].shape}\")  # Should be [16, 512], not [16, 1, 512]\n",
    "# BERT expects bs,max_seq_length\n",
    "\n",
    "# ========\n",
    "# SET UP\n",
    "# ========\n",
    "\n",
    "# Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84cab410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for i, label in enumerate(train_dataset.data['label'].value_counts().index)]\n",
    "labels\n",
    "label_0 = labels[1]\n",
    "label_4 = labels[0]\n",
    "label_0,label_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "134669f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating training time by running a few batches...\n",
      "============================================================\n",
      "Warming up...\n",
      "\n",
      "Timing 10 training batches...\n",
      "Timing 10 validation batches...\n",
      "\n",
      "============================================================\n",
      "TRAINING TIME ESTIMATE\n",
      "============================================================\n",
      "Training batches per epoch: 72,000\n",
      "Validation batches per epoch: 18,000\n",
      "Average time per training batch: 2.319s\n",
      "Average time per validation batch: 0.388s\n",
      "\n",
      "Time per epoch:\n",
      "  Training: 46h 22m 55s\n",
      "  Validation: 1h 56m 32s\n",
      "  Total: 48h 19m 27s\n",
      "\n",
      "Estimated total training time for 3 epochs: 144h 58m 23s\n",
      "============================================================\n",
      "\n",
      "Resetting model state after timing estimation...\n",
      "Ready to start training!\n"
     ]
    }
   ],
   "source": [
    "# Estimate Training Time\n",
    "import time\n",
    "\n",
    "print(\"Estimating training time by running a few batches...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save model state to restore after timing\n",
    "model_state = model.state_dict().copy()\n",
    "optimizer_state = optimizer.state_dict().copy()\n",
    "\n",
    "# Warm up (first batch is usually slower)\n",
    "print(\"Warming up...\")\n",
    "model.train()\n",
    "warmup_batch = next(iter(train_loader))\n",
    "input_ids = warmup_batch['input_ids'].to(device)\n",
    "attention_mask = warmup_batch['attention_mask'].to(device)\n",
    "labels = warmup_batch['label'].to(device)\n",
    "_ = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "# Time training batches\n",
    "num_test_batches = 10\n",
    "print(f\"\\nTiming {num_test_batches} training batches...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    if i >= num_test_batches:\n",
    "        break\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['label'].to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "avg_time_per_batch_train = train_time / num_test_batches\n",
    "\n",
    "# Time validation batches\n",
    "print(f\"Timing {num_test_batches} validation batches...\")\n",
    "model.eval()\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if i >= num_test_batches:\n",
    "            break\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "val_time = time.time() - start_time\n",
    "avg_time_per_batch_val = val_time / num_test_batches\n",
    "\n",
    "# Calculate estimates\n",
    "total_train_batches = len(train_loader)\n",
    "total_val_batches = len(val_loader)\n",
    "\n",
    "time_per_epoch_train = avg_time_per_batch_train * total_train_batches\n",
    "time_per_epoch_val = avg_time_per_batch_val * total_val_batches\n",
    "time_per_epoch_total = time_per_epoch_train + time_per_epoch_val\n",
    "total_training_time = time_per_epoch_total * EPOCHS\n",
    "\n",
    "# Convert to readable format\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs}s\"\n",
    "    else:\n",
    "        return f\"{secs}s\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING TIME ESTIMATE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training batches per epoch: {total_train_batches:,}\")\n",
    "print(f\"Validation batches per epoch: {total_val_batches:,}\")\n",
    "print(f\"Average time per training batch: {avg_time_per_batch_train:.3f}s\")\n",
    "print(f\"Average time per validation batch: {avg_time_per_batch_val:.3f}s\")\n",
    "print(f\"\\nTime per epoch:\")\n",
    "print(f\"  Training: {format_time(time_per_epoch_train)}\")\n",
    "print(f\"  Validation: {format_time(time_per_epoch_val)}\")\n",
    "print(f\"  Total: {format_time(time_per_epoch_total)}\")\n",
    "print(f\"\\nEstimated total training time for {EPOCHS} epochs: {format_time(total_training_time)}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Restore model and optimizer state (reset after timing test)\n",
    "print(\"\\nResetting model state after timing estimation...\")\n",
    "model.load_state_dict(model_state)\n",
    "optimizer.load_state_dict(optimizer_state)\n",
    "print(\"Ready to start training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12ceae",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b585736c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n\u001b[32m     18\u001b[39m loss = outputs.loss\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m optimizer.step()\n\u001b[32m     22\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI/NLP using PyTorch/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI/NLP using PyTorch/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GenAI/NLP using PyTorch/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')  # Initialize best_val_loss to a very high value\n",
    "best_epoch = -1  # Initialize best_epoch to an invalid value to track the epoch of the best validation loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    # Training with progress bar\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{EPOCHS} [Train]', leave=False)\n",
    "    for batch in train_pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation with progress bar\n",
    "    model.eval()\n",
    "    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch + 1}/{EPOCHS} [Val]', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar with current loss\n",
    "            val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    # Check if the current validation loss is the lowest; if so, save the model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Print the best epoch and its validation loss\n",
    "print(f\"The lowest validation loss was {best_val_loss:.4f} at epoch {best_epoch + 1}\")\n",
    "\n",
    "# Load the best model before calculating final metrics\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "train_accuracy = calculate_accuracy(model, train_loader, device)\n",
    "train_precision = calculate_precision_recall_f1(model, train_loader, device, label_0, label_4)\n",
    "val_accuracy = calculate_accuracy(model, val_loader, device)\n",
    "val_precision = calculate_precision_recall_f1(model, val_loader, device, label_0, label_4)\n",
    "print(f'Best Model Training Accuracy: {train_accuracy:.2f}%')\n",
    "print(f'Best Model Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Print precision, recall, and F1 scores\n",
    "print(f'\\nTraining Metrics:')\n",
    "print(f\"  Label {label_0} - Precision: {train_precision['precision_label_0']:.4f}, Recall: {train_precision['recall_label_0']:.4f}, F1: {train_precision['f1_label_0']:.4f}\")\n",
    "print(f\"  Label {label_4} - Precision: {train_precision['precision_label_4']:.4f}, Recall: {train_precision['recall_label_4']:.4f}, F1: {train_precision['f1_label_4']:.4f}\")\n",
    "\n",
    "print(f'\\nValidation Metrics:')\n",
    "print(f\"  Label {label_0} - Precision: {val_precision['precision_label_0']:.4f}, Recall: {val_precision['recall_label_0']:.4f}, F1: {val_precision['f1_label_0']:.4f}\")\n",
    "print(f\"  Label {label_4} - Precision: {val_precision['precision_label_4']:.4f}, Recall: {val_precision['recall_label_4']:.4f}, F1: {val_precision['f1_label_4']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2251474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and plot confusion matrices\n",
    "# Map labels to class names (0 = Negative, 4 = Positive based on your assignment)\n",
    "label_to_name = {0: 'Negative', 4: 'Positive'}\n",
    "unique_labels = sorted(train['label'].unique())\n",
    "class_names = [label_to_name.get(label, f'Label {label}') for label in unique_labels]\n",
    "\n",
    "# Confusion Matrix for Validation Set\n",
    "print(\"Validation Set Confusion Matrix:\")\n",
    "y_true_val, y_pred_val = get_confusion_matrix(model, val_loader, device)\n",
    "cm_val = plot_confusion_matrix(y_true_val, y_pred_val, class_names, \n",
    "                                title=\"Validation Set - Confusion Matrix\")\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nTest Set Confusion Matrix:\")\n",
    "y_true_test, y_pred_test = get_confusion_matrix(model, test_loader, device)\n",
    "cm_test = plot_confusion_matrix(y_true_test, y_pred_test, class_names, \n",
    "                                 title=\"Test Set - Confusion Matrix\")\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = calculate_accuracy(model, test_loader, device)\n",
    "print(f'\\nTest Set Accuracy: {test_accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
