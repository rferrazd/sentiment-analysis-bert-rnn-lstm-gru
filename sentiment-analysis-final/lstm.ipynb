{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0fc5ed",
   "metadata": {},
   "source": [
    "# LSTM \n",
    "\n",
    "Implementation based on code from notebook: 1. Encoder-Decoder Seq2Seq.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc8885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import torch \n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load the datasets \n",
    "with open(\"train.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"val.pkl\", \"rb\") as f:\n",
    "    val = pickle.load(f)\n",
    "with open(\"test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "num_labels = train['label'].nunique()\n",
    "print(\"Number of labels: \", num_labels)\n",
    "labels = [label for i, label in enumerate(train['label'].value_counts().index)]\n",
    "labels\n",
    "label_0 = labels[1]\n",
    "label_4 = labels[0]\n",
    "print(f\"Label 0: {label_0} and label 4: {label_4}\")\n",
    "\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6944bc",
   "metadata": {},
   "source": [
    "# LSTM Encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d86429",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define encoder class\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # input tensor: (bs, seq_len) - tensor will contain indexes for the embedding module\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        \n",
    "        # Make LSTM bidirectional\n",
    "        # (bs, seq_len, hidden_size) \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        # NOTE:\n",
    "        # when the LSTM is bidirectional, the output size from the LSTM is hidden_size * 2\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.mlp = nn.Linear(hidden_size**2, ?)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        embedded = self.dropout(embeddings)\n",
    "        print(\"Embedding shape: \", embedded.shape)\n",
    "        # When LSTM is bidirectional, the output, hidden and cell state will be for both directions\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        output = self.mlp(output)\n",
    "        # output.shape = (bs, seq_len, _? 512 )\n",
    "        return output, (hidden, cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b4420ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5, 15, 25, 11, 14, 24, 20, 35,  7, 24],\n",
      "        [ 9, 31,  2, 11, 17, 37, 14, 24, 12, 18],\n",
      "        [23,  2, 21,  8, 36, 30,  8, 25, 34, 33],\n",
      "        [ 0, 18,  7, 30, 39, 22, 16, 39, 26, 25]])\n",
      "Embedding shape:  torch.Size([4, 10, 256])\n",
      "torch.Size([4, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256 \n",
    "bs = 4\n",
    "seq_len = 10\n",
    "vocab_size = 40\n",
    "\n",
    "lstm = Encoder(\n",
    "    input_size = vocab_size, \n",
    "    hidden_size = hidden_size\n",
    ")\n",
    "\n",
    "# Create a batch of random indices (simulating tokenized word ids)\n",
    "# Embedding expects input of dtype torch.long (not float)\n",
    "t = torch.randint(low=0, high=vocab_size, size=(bs, seq_len), dtype=torch.long)\n",
    "print(t)\n",
    "out, (hidden, cell) = lstm(t)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550ce2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
